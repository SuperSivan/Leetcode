[牛客机器学习算法](https://www.nowcoder.com/tutorial/95/f2446e6a55c244859d7a9bd0b24a6650)

[计算机视觉算法岗面经：两万字啊你确定不看看](https://www.nowcoder.com/discuss/128148?type=post&order=time&pos=&page=3&ncTraceId=&channel=-1&source_id=search_post_nctrack&subType=2)


1. 手写K-means
  - https://www.cnblogs.com/pinard/p/6164214.html
    1. 随机选择k个初始作为质心
    2. 计算样本到质心的距离，将样本分为K类
    3. 每类中求均值得到每类新的质心
    4. 重复2-3，直到均值不更新
 - k-means++:

    可以直观地将这改进理解成这K个初始聚类中心相互之间应该分得越开越好。一开始只选择一个初始点，然后按照这个原则，知道选好k个点。https://www.cnblogs.com/yixuan-xu/p/6272208.html

2. 手写最小二乘
 - https://www.cnblogs.com/iamccme/archive/2013/05/15/3080737.html
  - https://blog.csdn.net/yhao2014/article/details/51491413
  - https://www.cnblogs.com/zhangli07/p/12013561.html
```c++
#include <iostream>
using namespace std;

void LinearFit(double abr[], double x[], double y[], int n) {//线性拟合ax+b
    double xsum, ysum, x2sum, xysum;
    xsum = 0; ysum = 0; x2sum = 0; xysum = 0;
    for (int i = 0; i < n; i++)
    {
        xsum += x[i];
        ysum += y[i];
        x2sum += x[i] * x[i];
        xysum += x[i] * y[i];
    }
    abr[0] = (n * xysum - xsum * ysum) / (n * x2sum - xsum * xsum);//a
    abr[1] = (ysum - abr[0] * xsum) / n;//b
}

int main()
{
    int const N = 6;//12;

    double x[N] = { 1,2,3,4,5,5.6};
    double y[N] = { 1,2,3,4,5,5.8};
    double abr[2];

    LinearFit(abr, x, y, N);

    cout << showpos;//显示正负号
    cout << "相关系数拟合直线:y=" << abr[0] << "x" << abr[1] << endl;
    system("pause");
    return 0;
}
```
3. 最小二乘与梯度下降的区别
 - https://www.cnblogs.com/iamccme/archive/2013/05/15/3080737.html
 
3. SVM

    支持向量机为一个二分类模型,它的基本模型定义为特征空间上的间隔最大的线性分类器。而它的学习策略为最大化分类间隔,最终可转化为凸二次规划问题求解。

    核函数隐含着一个从低维空间到高维空间的映射,这个映射可以把低维空间中线性不可分的两类点变成线性可分的。

    当数据的特征提取的较好,所包含的信息量足够大,很多问题是线性可分的那么可以采用**线性核**。若特征数较少,样本数适中,对于时间不敏感,遇到的问题是线性不可分的时候可以使用**高斯核**来达到更好的效果。

    手推SVM：https://www.nowcoder.com/tutorial/95/4dae460ff4244146b0918594d9b38f09

4. [决策树](https://easyai.tech/ai-definition/decision-tree/)


    决策树算法采用树形结构，使用层层推理来实现最终的分类。

    特征选择决定了使用哪些特征来做判断。在训练数据集中，每个样本的属性可能有很多个，不同属性的作用有大有小。因而特征选择的作用就是筛选出跟分类结果相关性较高的特征，也就是分类能力较强的特征。在特征选择中通常使用的准则是：**信息增益**。

    决策树学习的三个步骤（**ID3**树为例）：

    1. 特征选择:

        特征选择决定了使用哪些特征来做判断。在训练数据集中，每个样本的属性可能有很多个，不同属性的作用有大有小。因而特征选择的作用就是筛选出跟分类结果相关性较高的特征，也就是分类能力较强的特征。在特征选择中通常使用的准则是：信息增益。
    2. 决策树生成:

        选择好特征后，就从根节点触发，对节点计算所有特征的信息增益(机器学习P75)，选择信息增益最大的特征作为节点特征，根据该特征的不同取值建立子节点；对每个子节点使用相同的方式生成新的子节点，直到信息增益很小或者没有特征可以选择为止。
    
    3. 决策树剪枝:

        剪枝的主要目的是对抗「过拟合」，通过主动去掉部分分支来降低过拟合的风险。
5. 随机森林

    当我们进行分类任务时，新的输入样本进入，就让森林中的每一棵决策树分别进行判断和分类，每个决策树会得到一个自己的分类结果，决策树的分类结果中哪一个分类最多，那么随机森林就会把这个结果当做最终的结果。
6. [DBSCAN](https://blog.csdn.net/huacha__/article/details/81094891)
    - 如果圈（半径预设）里面的点的数量足够多（数量阈值预设），那么就继续圈下去，直到圈里面的数量不够多
7. [IOU计算](https://frank909.blog.csdn.net/article/details/91366128?utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7EBlogCommendFromMachineLearnPai2%7Edefault-1.control)

8. ResNet介绍
 - ResNets要解决的是深度神经网络的“退化”问题。我们知道，对浅层网络逐渐叠加layers，模型在训练集和测试集上的性能会变好，因为模型复杂度更高了，表达能力更强了，可以对潜在的映射关系拟合得更好。而“退化”指的是，给网络叠加更多的层后，性能却快速下降的情况。
- 为什么要跳过连接？
    1. 他们通过允许梯度通过这条可选的捷径来缓解梯度消失的问题
    2. 它们允许模型学习一个恒等函数，该函数确保高层的性能至少与低层一样好，而不是更差。
9. BN
 - 因为深层神经网络在做非线性变换前的激活输入值（就是那个x=WU+B，U是输入）随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这导致反向传播时低层神经网络的梯度消失，这是训练深层神经网络收敛越来越慢的本质原因，而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，使得非线性变换函数的输入值落入对输入比较敏感的区域，以此避免梯度消失问题。

 10. 过拟合

- 过拟合的原因：

    1. 训练数据集样本单一，样本不足
    2. 训练数据中噪声干扰过大
    3. 模型过于复杂
- 解决过拟合方法：
    1. 数据增强
    2. 采用合适的模型（控制模型的复杂度）
    3. 删除冗余特征，降低特征的数量
    4. L1 / L2 正则化：使得权重参数 w变小，为什么能防止过拟合呢？因为更小的权重参数w意味着模型的复杂度更低，对训练数据的拟合刚刚好，不会过分拟合训练数据，从而提高模型的泛化能力。
    5. Dropout
    6. 提前终止

11. 激活函数
- 激活函数给神经元引入了非线性的因素，使得神经网络可以逼近任何非线性函数

    1. [sigmoid](https://blog.csdn.net/xiaomifanhxx/article/details/82828548)
    - 输出在(0,1)之间，它可以将一个实数映射到(0,1)的范围内，可以用来做二分类。常用于:在特征相差比较复杂或是相差不是特别大的时候效果比较好。
    -  sigmoid函数的缺点:1)激活函数的计算量大，反向传播求误差梯度时，求导涉及到除法。2)反向传播的时候，很容易出现梯度消失的情况（根据求导公式），从而无法完成深度神经网络的训练；
    2. [relu](https://www.cnblogs.com/lliuye/p/9486500.html)
    - 在输入为正数的时候（对于大多数输入 z 空间来说），不存在梯度消失问题。计算速度要快很多。ReLU函数只有线性关系，不管是前向传播还是反向传播，都比sigmod和tanh要快很多
    - 当输入为负时，梯度为0，会产生梯度消失问题。
    - Leaky ReLU函数解决了ReLU函数在输入为负的情况下产生的梯度消失问题。
12. [梯度消失/爆炸](https://zhuanlan.zhihu.com/p/180568816)
- 多个激活函数偏导数的连乘，和多个权重参数的连乘。如果激活函数求导后与权重相乘的积大于1，那么随着层数增多，求出的梯度更新信息将以指数形式增加，即发生**梯度爆炸**；如果此部分小于1，那么随着层数增多，求出的梯度更新信息将会以指数形式衰减，即发生**梯度消失**。
- 解决方法：
    1. 预训练+微调（pre-training + fine-tunning）
    2. 正则
    3. 合理的激活函数（relu）
    4. BN
    5. resnet
13. [optimizer](https://zhuanlan.zhihu.com/p/150113660)


问了relu，sigmoid的应用场景，sigmoid的缺点，数学上解释为什么会这样。
如何根治梯度消食（答曰没办法根治，只能一定程度缓解）。
卷积的实现方式。


BN介绍，几个参数，为什么要加缩放和偏置
用c++手写非极大值抑制（NMS），注意边界处理

训练设备，显卡，显存，占用多少，是否做过多卡并行训练

查看显卡占用率的命令
泰勒展开
写交叉熵公式

梯度消失原因
